import os
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

os.environ["OPENAI_API_KEY"] = ""

st.subheader("ğŸ’¬ AI Chatbot")

# 1) ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "history" not in st.session_state:
    st.session_state.history = ''

def route(info):
    # ìœ íŠœë¸Œ ê²€ìƒ‰
    if "ìœ íŠœë¸Œ" in info["topic"]:
        return "ìœ íŠœë¸Œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ë  ì˜ˆì •ì…ë‹ˆë‹¤"

    # ê°€ê²© ê²€ìƒ‰
    elif "ê°€ê²©ê²€ìƒ‰" in info["topic"]:
        return "ê°€ê²© ê²€ìƒ‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ë  ì˜ˆì •ì…ë‹ˆë‹¤"
    
    else : 
        general_chain = (
            PromptTemplate.from_template(
            """
                ë‹¹ì‹ ì€ ì™€ì¸ì„ ì¶”ì²œí•´ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤. ê³¼ê±° ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.

                <question>
                {question}
                </question>

                <history>
                {history}
                </history>
            """
            )
            | ChatOpenAI(model="gpt-4o-mini")
            | StrOutputParser()
        )
        return general_chain

def generate_summary(summary, new_lines): 
    summary_chain = (
        PromptTemplate.from_template(
            """
            ì œê³µëœ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ì´ì „ì˜ ìš”ì•½ì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            <ì˜ˆì‹œ>
            í˜„ì¬ ìš”ì•½: ì¸ê°„ì´ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ë¬´ì—‡ì„ ìƒê°í•˜ëŠ”ì§€ AIì—ê²Œ ë¬¼ì–´ë´…ë‹ˆë‹¤. AIëŠ” ì¸ê³µì§€ëŠ¥ì´ ì„ í•œ í˜ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
            ìƒˆë¡œìš´ ëŒ€í™” ë¼ì¸:
            ì¸ê°„: ì™œ ì¸ê³µì§€ëŠ¥ì´ ì„ í•œ í˜ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?
            AI: ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì´ ê·¸ë“¤ì˜ ì ì¬ë ¥ì„ ì™„ì „íˆ ë°œíœ˜í•˜ë„ë¡ ë„ìš¸ ê²ƒì…ë‹ˆë‹¤.
            ìƒˆ ìš”ì•½:\nì¸ê°„ì´ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ë¬´ì—‡ì„ ìƒê°í•˜ëŠ”ì§€ AIì—ê²Œ ë¬¼ì–´ë³´ê³ , AIëŠ” ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì ì¬ë ¥ì„ ì™„ì „íˆ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ ë•ê¸° ë•Œë¬¸ì— ì„ í•œ í˜ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
            </ì˜ˆì‹œ>
            í˜„ì¬ ìš”ì•½:
            {summary}
            ìƒˆë¡œìš´ ëŒ€í™” ë¼ì¸:
            {new_lines}
            ìƒˆ ìš”ì•½:
            """
        )
        | ChatOpenAI(model="gpt-4o-mini", temperature=0)
        | StrOutputParser()
    )
    return summary_chain.invoke({'summary' : summary, 'new_lines' : new_lines})

def generate_response(query, history) :
    router_prompt = PromptTemplate.from_template(
        """
        ì£¼ì–´ì§„ ì‚¬ìš©ì ì§ˆë¬¸ì„ `ê°€ê²©ê²€ìƒ‰`, `ìœ íŠœë¸Œ` ë˜ëŠ” `ê¸°íƒ€` ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”. í•œ ë‹¨ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

        ê°€ê²©ê²€ìƒ‰: ì™€ì¸ ê°€ê²© ë° êµ¬ë§¤ ì •ë³´ ì§ˆë¬¸
        ìœ íŠœë¸Œ: ë™ì˜ìƒì´ë‚˜ ìœ íŠœë¸Œ ê´€ë ¨ ì§ˆë¬¸
        ê¸°íƒ€: ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ ë˜ëŠ” ìœ„ ë²”ì£¼ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì§ˆë¬¸

        <question>
        {question}
        </question>

        Classification:
        """
    )
    router_chain = router_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0) | StrOutputParser()
    full_chain = (
        {
            'topic' : router_chain,
            "question" : lambda x : x["question"],
            "history" : lambda x : x["history"]
        }
        | RunnableLambda(route)
    )
    print('history : ', history)
    response = full_chain.invoke({"question" : query, "history" : history})
    print(response)
    new_lines = f'ì¸ê°„ : {query} \n AI : {response} '
    new_summary = generate_summary(st.session_state.history, new_lines)
    st.session_state.history = new_summary
    return response

# 2) ì‚¬ìš©ì ì…ë ¥
prompt = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

# 3) ë“¤ì–´ì˜¨ ê°’ ì €ì¥
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.spinner("AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        response = generate_response(prompt, st.session_state.history)
    st.session_state.messages.append({"role": "ai", "content": response})

# 4) ì¶œë ¥
for message in st.session_state.messages :
    with st.chat_message(message["role"]):
        st.markdown(message["content"])